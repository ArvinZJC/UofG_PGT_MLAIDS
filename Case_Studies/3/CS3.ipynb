{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML(H)\n",
    "## John Williamson 2019/2020\n",
    "\n",
    "---\n",
    "\n",
    "# Visualisation in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "In this lab we will explore how we can use visualisation to explore data before we begin training models. We will do this in the context of a challenge to classify audio data.\n",
    "\n",
    "### Task\n",
    "The task involves optimising the pre-processing of features for a simple classification problem, using visualisation (unsupervised learning) to help guide you in choosing good feature vector selection. The idea is to get *insight* into the data **using visualisation** to create effective machine learning models.\n",
    "\n",
    "### Classifier\n",
    "We will only use the most basic of classifiers in this example -- **k nearest neighbours** -- and focus on how to process the features to make this simple algorithm work as effectively as possible. Many modern approaches would use deep learning to learn an end-to-end classifier that infers appropriate feature extraction directly.\n",
    "\n",
    "Everything to do the analysis and testing is provided for you. Your task is to modify the parameters that affect the creation of the feature vectors (see the green highlighted box below) and explore different visualisations to help identify a good feature transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import audio_task\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "This topic is essentially the operations behind the **Stane** project [Paper](http://www.dcs.gla.ac.uk/~rod/publications/MurWilHugQua08.pdf) [Video](http://www.dcs.gla.ac.uk/~rod/Videos/i_chi2.mov)\n",
    "\n",
    "\n",
    "This used 3D printed textures on mobile devices. Scratching the fingernail across the surface generates distinctive sounds, which are propagated through the case and picked up by a piezo microphone. Different regions have different textures, and thus the area being rubbed can be determined by analysing the audio signal.\n",
    "\n",
    "<img src=\"imgs/piezo.png\" width=\"400px\">\n",
    "<img src=\"imgs/stane_real.png\" width=\"400px\">\n",
    "<!-- <img src=\"imgs/shell.png\" width=\"400px\">\n",
    "<img src=\"imgs/disc.png\" width=\"400px\"> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature vectors\n",
    "In almost all machine learning contexts, we predict outputs given a **fixed length** set of features; the input space a fixed dimension $d$. Each of those features is usually (but not always) continuous-valued.\n",
    "\n",
    "Sometimes the data fall naturally into this space (e.g. classifying the iris type by 3 physical measurements). In cases such as in audio classification, though, we want to make predictions based on *time series*; a set of measurements of the same variable set made repeatedly over time.\n",
    "\n",
    "#### Windowing\n",
    "One general solution to this time series problem is to break a sequence up into a fixed length sequence of previous measurements. For example the measurements $[x_{t=t}, x_{t=t-1}, x_{t=t-2}, \\dots, x_{t=t-d}]$ might make up the feature vector. This process is known as *windowing*, because we chop up the data into fixed length windows by \"sliding\" a time window along the data. **Consecutive (but possible discontiguous or overlapping) windows are almost universally used in audio contexts.**\n",
    "\n",
    "<img src=\"imgs/contiguous_windows.png\">\n",
    "<img src=\"imgs/overlapping_windows.png\">\n",
    "\n",
    "These windows can overlap, which increases the size of the training set, but excessive overlapping can capture lots of redundant features examples. This can increase overfitting and training time without improving the classifier performance. Balancing the size of the windows (and thus the feature vector size $d$) and the amount of overlap is a matter of experimentation and domain knowledge.\n",
    "\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transforms\n",
    "\n",
    "Often the \"natural\" raw form of the data can be difficult to classify. This might be because it has very high dimension, it is very noisy, or the classification boundary just isn't very compatible with your classifier (e.g. the class borders in the original space are highly-nonlinear and you are using a linear classifier). \n",
    "\n",
    "**Feature engineering** is the art of finding transforms of the raw data that increase classifier performance. These can often be simple, such as dropping some of the measurements entirely (under the assumption that they are irrelvant), or averaging measurements together (under the assumption that this reduces noise).\n",
    "\n",
    "### Audio transforms\n",
    "**Audio** data tends to be very high dimensional -- you might get 4000 to 44100 measurements for a single second of data.  A single audio sample has very little information indeed; it is the longer-term (millisecond to second) properties that have all the interesting information. \n",
    "\n",
    "So want transforms that pull out interesting features **over time**. The classical feature transform is the **Fourier transform**, which rewrites a signal varying over time as a sum of sinusoidal (periodic) components. This functions much like our ear works, splitting up audio in **frequency bands**, each of which has a **phase** and an **amplitude**. Many of these are already implemented in standard libraries, like the Fast Fourier Transform (fft) and the Discrete Cosine Transform (dct). We will use these in our classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The challenge\n",
    "<a id=\"challenge\"></a>\n",
    "You have to use visualisation strategies to choose a feature vector transform for a classifier that classifies the region of a device that is being touched based on the sound recorded from a piezo contact microphone. There are four possible touch regions and also a silence/handling noise class (class 0):\n",
    "\n",
    "<img src=\"imgs/regions.png\"> \n",
    "\n",
    "The data are all in the `data/` folder.\n",
    "\n",
    "You have training data for these regions, `challenge_train_{01234}.wav`\n",
    "\n",
    "See the code below which plots test datasets consisting of labeled mixtures of these classes alongside your predicted labels.  The code below loads the data from 4Khz 16 bit mono wavefiles into a collection of features vectors `X` and a corresponding set of labels `y`.  \n",
    "\n",
    "### Feature selection\n",
    "You can modify the feature selection properties set below to adjust the results. **The rest of the pipeline is fixed  -- you cannot modify the classifier itself in this exercise.**\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "This cell below is the only control you have over the performance of the classification. You will use the visualisation techniques to explore how these transforms change the data you get. NOTE: you may well not understand the meaning of the parameters. Use the visualisations to help guide you to a good choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### PARAMETERS ##################\n",
    "# You can change these values\n",
    "# 1 sample = 1/4096 of a second\n",
    "\n",
    "parameters = {\n",
    "'size':256, # how long each window is, in samples\n",
    "'step':128, # how many samples to move forward in each window\n",
    "'decimate':1, # how much reduction of the feature vector to perform; integer = 1,2,3,4,5...\n",
    "'feature_range' :(0.0, 1.0), # what slice of the feature vectors to take (0.0, 1.0)=whole vector\n",
    "# must be between 0.0 and 1.0 for the start and end \n",
    "'window_fn':\"boxcar\", # one of window_fns (below)\n",
    "'feature_fn':\"raw\" # one of feature_fns    \n",
    "}\n",
    "#####################################\n",
    "\n",
    "# valid feature_fns: ['dct', 'fft', 'fft_phase', 'dct_phase', 'cepstrum', 'raw'])\n",
    "# valid window_fns: ['hamming', 'hann', 'boxcar', 'blackmanharris']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to (re)load the data into a set of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e59924eb88ab950c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = audio_task.load_data(parameters)\n",
    "print(\"X shape\", X.shape, \"y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have UMAP installed, uncomment and run the line below (once only)\n",
    "#!pip install --no-cache --user umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "We can visualise these features. Experiment with using:\n",
    "* PCA `sklearn.decomposition.pca`\n",
    "* LLE `sklearn.manifold.LocallyLinearEmbedding`\n",
    "* ISOMAP `sklearn.manifold.Isomap`\n",
    "* tSNE `sklearn.manifold.TSNE`\n",
    "* UMAP `umap`\n",
    "\n",
    "You can use alternative visualisation methods if you wish. **You may need to subsample the features to visualise in reasonable time.**. For example, you can subsample to just every 10th feature vector using `X[::10, :]`\n",
    "\n",
    "Use the visualisations to find a feature vector selection that you expect to perform well with a a kNN classifier. Use the visualisation to explore these transforms rather than trial and error with the classifier itself.\n",
    "\n",
    "<div class=\"alert alert-box alert-success\"> \n",
    "\n",
    "**Task**: iterate over visualisations and tweaks to the feature preprocessing to see if you can find a structure that separates classes well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition, sklearn.manifold, umap\n",
    "\n",
    "# example showing PCA of features\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# create a PCA object\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "\n",
    "# transform every fifth vector (to save computation time)\n",
    "# some techniques need this; others will be fast enough\n",
    "subsample = 5\n",
    "pca_X = pca.fit_transform(X[::subsample, :])\n",
    "\n",
    "# show a scatter plot of the first two dimensions\n",
    "scatter = ax.scatter(\n",
    "    pca_X[:, 0],\n",
    "    pca_X[:, 1],\n",
    "    c=y[::subsample],\n",
    "    cmap=\"tab10\",\n",
    "    vmin=0,\n",
    "    vmax=10,\n",
    "    alpha=0.7,\n",
    "    s=8,\n",
    ")\n",
    "\n",
    "\n",
    "##### Make the figure look a bit cleaner\n",
    "# Produce a legend for the ranking (colors). Even though there are 40 different\n",
    "# rankings, we only want to show 5 of them in the legend.\n",
    "legend1 = ax.legend(*scatter.legend_elements(num=4), loc=\"upper left\", title=\"Class\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "ax.axhline(0, color=\"k\", alpha=0.1)\n",
    "ax.axvline(0, color=\"k\", alpha=0.1)\n",
    "\n",
    "ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "The code below creates a standard KNN classifier which predicts the output class based on the 7 nearest elements in feature vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ae1d5ba9b2c8f88",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# fit a classifier\n",
    "classifier = audio_task.knn_fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Visualising predictions \n",
    "You can preview your results with a separate (non-secret) test set which is provided. This will show you a visualisation of your classifier versus the ground truth. Each visualisation shows the true activity (below the axis) as coloured blocks and the predicted activity (from the classifier) as coloured blocks above the axis.\n",
    "\n",
    "In the case of perfect classification, the blocks above and below the line will be exactly the same. In practice, there will always be significant noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage -- show results for all of the test cases\n",
    "for i in range(5):\n",
    "    audio_task.plot_test(classifier, parameters, f\"data/challenge_test_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final test function\n",
    "There is a test function `challenge_evaluate_performance(classifier_fn)`. This gives you your total current score. You don't have access to the internals of this function. See below for how to use it.\n",
    "The test takes some time to run; so you must be parsimonious with your calls to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a4804e23ef89aa4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Shows how to evaluate your performance\n",
    "audio_task.run_secret_test(classifier, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
