{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "In this lab, you will have the opportunity to explore Reinforcement Learning (RL) algorithms in practice, training agents to navigate through a 2D grid world to maximize their rewards. In a sequence of tasks, you will \n",
    "\n",
    "1. discover the effect of randomness on RL training,\n",
    "2. compare different RL algorithms trained on the same task, \n",
    "3. implement the Q-Learning algorithm\n",
    "4. explore whether the task is harder to learn if the agent started in a different random position in each episode, and\n",
    "5. explore different tasks by modifying the environment's reward structure\n",
    "\n",
    "## Setup\n",
    "\n",
    "We start by importing dependencies and setting up some functions that will be useful in completing these tasks.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from grid_world import GridWorld # ensure the file grid_world.py is located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The class below defines the grid world environment that we've already seen in the lecture. It consists of a grid with 4 rows and 5 columns, specifying 20 environment states (one state for each grid cell). In each episode, the agent starts in state (0, 3). The cell (4, 0) is declared a terminal state, ending an episode whenever the agent transitions into that state. All state transitions yield a reward of 0 by default. The reward for transitions into two specific states are set differently, yielding a reward of 1 when an agent's action results in a transition into state (4, 0), and yielding a reward of -1 when an agent's action results in a transition into state (1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleGrid(GridWorld):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(rows=4, cols=5, agent_start=(0, 3))\n",
    "        #self.rewards = -np.ones_like(self.rewards)\n",
    "        self.update_terminal(4, 0) # make state (4, 0) a terminal state\n",
    "        self.update_reward(4, 0, 1) # make transitions into state (4, 0) yield a reward of 1\n",
    "        self.update_reward(1, 2, -1) # make transitions into state (1, 2) yield a reward of -1\n",
    "        \n",
    "env = ExampleGrid() # instantiate the example grid\n",
    "env.render() # display the environment state(green dot), terminal states (gray shaded cells), and rewards (numbers inside cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Softmax Agent\n",
    "\n",
    "This agent class implements an explicit policy where the probability distribution over actions given a state $\\pi(a|s)$ is defined as the softmax over action-specific 'logits' $z$:\n",
    "\\begin{equation}\n",
    "\\pi(a=a_i|s) = \\sigma(z_{s,a}) = \\frac{e^{z_{s, a_i}}}{\\sum_j e^{z_{s, a_j}}}\n",
    "\\end{equation}\n",
    "\n",
    "Using a tabular state representation, the policy is parameterized with a separate parameter for each state-action pair $z_{s, a}$ with state $s: (x, y) \\in \\mathcal{S}$ and action $a \\in \\mathcal{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAgent():\n",
    "    \"\"\"\n",
    "    A tabular agent parameterizing the logits of a softmax policy.\n",
    "    \n",
    "    Args:\n",
    "        z (np.array): Parameters organized by [y, x, a], where (y, x) encode\n",
    "        the state (as a cell in a grid world) and 'a' each possible action.\n",
    "    \"\"\"\n",
    "    def __init__(self, z):\n",
    "        self.z = z\n",
    "        \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\" sample from the policy pi(a, s), where pi is defined as the \n",
    "        softmax function with parameters self.z[y, x].\"\"\"\n",
    "        x, y = state\n",
    "        p = np.exp(self.z[y, x])/np.sum(np.exp(self.z[y, x]))\n",
    "        a = np.random.choice(len(p), p=p)\n",
    "        return a\n",
    "    \n",
    "    def rollout(self, env, epsilon=0):\n",
    "        \"\"\" complete a full episode interacting with the environment \n",
    "        by following the current policy.\"\"\"\n",
    "        \n",
    "        states, actions, rewards = [], [], []\n",
    "        done = False\n",
    "\n",
    "        s = env.reset()\n",
    "        states.append(s)\n",
    "        while not done:\n",
    "            a = self.act(s, epsilon)\n",
    "            s, r, done, _ = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "        \n",
    "        return states, actions, rewards\n",
    "    \n",
    "    def render_pi(self, env, ax=None):\n",
    "        \"\"\" Display the learned policy. \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = env._create_fig()\n",
    "        env.render_grid(ax)\n",
    "        pi = np.exp(self.z) / np.expand_dims(np.sum(np.exp(self.z), axis=2), axis=2)\n",
    "        for y in range(env.rows):\n",
    "            for x in range(env.cols):\n",
    "                # render distribution over actions\n",
    "                p =  pi[y, x]\n",
    "                p_max = np.max(p)\n",
    "                for a in range(len(env.action_space)):\n",
    "                    length = p[a]/(1e-6+p_max)*0.3\n",
    "                    env.render_action(ax, action=env.action_space[a], agent=(x, y), length=length, hw=0.1, hl=0.15, ox=0, oy=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### 1. How does randomness affect training of REINFORCE ?\n",
    "\n",
    "The code snippet below trains a Tabular Softmax policy with REINFORCE for 1000 rollouts (episodes) and then plots the return of each rollout. It also illustrates the trained policy and shows an example rollout from the last training step.\n",
    "\n",
    "Write a function that repeats training 5 times and then plots mean, min and max reward after each rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate environment\n",
    "env = ExampleGrid()\n",
    "# instantiate policy\n",
    "logits = np.ones((env.rows, env.cols, len(env.action_space)))\n",
    "agent = SoftmaxAgent(z=logits)\n",
    "# define learning rate\n",
    "eta = 0.01\n",
    "gamma = 0.95\n",
    "\n",
    "# train for 300 episodes and record episode returns\n",
    "returns = []\n",
    "for i in range(1000):\n",
    "    \n",
    "    states, actions, rewards = agent.rollout(env)\n",
    "    G = np.sum(rewards* gamma**np.arange(len(rewards))) # here, the decay gamma is omitted, implicitly assuming it to be 1.\n",
    "    returns.append(G)\n",
    "    # question for students: why do we subtract the mean of past returns for training?\n",
    "    G -= np.mean(returns)\n",
    "    z = agent.z\n",
    "    dz = np.zeros_like(z)\n",
    "    for t in range(len(states)-1):\n",
    "        \n",
    "        x, y = states[t]\n",
    "        a = actions[t]\n",
    "        # estimate dz as (delta_a - 1)\n",
    "        dz[y,x] -= np.ones(4)\n",
    "        dz[y,x, a] += 1\n",
    "\n",
    "    # scale the gradient by the episode return\n",
    "    dz = dz * G\n",
    "    # apply gradient with learning rate eta\n",
    "    z = agent.z + eta * dz\n",
    "    # saturate logits to avoid distribution collapse\n",
    "    agent.z = np.maximum(-6, np.minimum(6, z))\n",
    "\n",
    "#agent.render_pi(env)\n",
    "#plt.title(f'REINFORCE policy after rollout {i+1}')\n",
    "returns_reinforce = returns\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 24))\n",
    "ax[0].plot(returns_reinforce)\n",
    "ax[0].set_title('REINFORCE training return of each rollout')\n",
    "ax[0].set_ylabel('Return')\n",
    "ax[0].set_xlabel('Rollout')\n",
    "agent.render_pi(env, ax[1])\n",
    "ax[1].set_title('REINFORCE learned policy after training.')\n",
    "env.render_rollout(states, ax=ax[2])\n",
    "ax[2].set_title('REINFORCE example rollout after training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How does Actor-Critic compare to REINFORCE in training performance?\n",
    "\n",
    "The code snippet below trains a Tabular Softmax policy with Actor-Critic for 1000 rollouts (episodes) and then plots the return of each rollout. It also illustrates the trained policy and shows an example rollout from the last training step.\n",
    "\n",
    "Train Actor-Critic 5 times and compare mean, min and max return in each rollout with the mean min and max return observed during REINFORCE training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic\n",
    "# instantiate environment\n",
    "env = ExampleGrid()\n",
    "# instantiate policy\n",
    "logits = np.ones((env.rows, env.cols, len(env.action_space)))\n",
    "agent = SoftmaxAgent(z=logits)\n",
    "# define learning rate\n",
    "eta = 0.2\n",
    "gamma = 1. # discount factor\n",
    "\n",
    "# initialize the critic\n",
    "Q = np.zeros_like(logits)\n",
    "\n",
    "returns = []\n",
    "for i in range(1000):\n",
    "    \n",
    "    states, actions, rewards = agent.rollout(env)\n",
    "    G = np.sum(rewards)\n",
    "    returns.append(G)\n",
    "    # question for students: why do we subtract the mean of past returns?\n",
    "    G -= np.mean(returns)\n",
    "    p = np.exp(agent.z) / np.expand_dims(np.sum(np.exp(agent.z), axis=2), axis=2)\n",
    "    dz = np.zeros_like(agent.z)\n",
    "    for t in range(len(states)-1):\n",
    "        \n",
    "        x, y = states[t]\n",
    "        a = actions[t]\n",
    "        \n",
    "        # estimate updated Q\n",
    "        x_next, y_next = states[t+1]\n",
    "        v = np.sum(Q * p, axis=2)[y_next, x_next]\n",
    "        Q[y, x, a] = rewards[t] + gamma * v\n",
    "        \n",
    "        # estimate gradient\n",
    "        # here, we scale gradients with the relevant Q-value in each step \n",
    "        # instead of using the episode return at the end.\n",
    "        dz[y, x] -= Q[y, x, a]\n",
    "        dz[y, x, a] += Q[y, x, a]\n",
    "    \n",
    "    # update parameters (question for students: what does the 0.99 accomplish?)\n",
    "    z = 0.99*agent.z + eta * dz\n",
    "    # saturate parameters to prevent distribution collapse\n",
    "    agent.z = np.maximum(-6, np.minimum(6, z))\n",
    "\n",
    "returns_ac = returns\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 24))\n",
    "ax[0].plot(returns_ac)\n",
    "ax[0].set_title('Actor-Critic training return of each rollout')\n",
    "ax[0].set_ylabel('Return')\n",
    "ax[0].set_xlabel('Rollout')\n",
    "agent.render_pi(env, ax[1])\n",
    "ax[1].set_title('Actor-CriticNFORCE learned policy after training.')\n",
    "env.render_rollout(states, ax=ax[2])\n",
    "ax[2].set_title('Actor-Critic example rollout after training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the Q-Learning algorithm\n",
    "\n",
    "\n",
    "### 3.1 Action Selection\n",
    "\n",
    "Implement the ```act``` method of the `QAgent` below to select the action with maximum Q-value. \n",
    "\n",
    "Amend your implementation to an $\\epsilon$-greedy policy that returns a random action with probability $\\epsilon$ and returns the action with maximum Q-value with probability $(1-\\epsilon)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(SoftmaxAgent):\n",
    "    \"\"\"\n",
    "    A tabular agent parameterizing the Q-values corresponding to the policy.\n",
    "    \n",
    "    Args:\n",
    "        q (np.array): Parameters organized by [y, x, a], where (y, x) encode\n",
    "                    the state (as a cell in a grid world) and 'a' each possible action.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "        \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \"\"\" select the action with maximum q-value for the given state.\n",
    "        \n",
    "        TODO: \n",
    "            1. return the action with maximum Q-value. Assume all relevant Q-values\n",
    "               are stored in self.q[y, x]. The function np.argmax returns the index \n",
    "               of the maximum value (consult the numpy documentation online for \n",
    "               examples).\n",
    "               \n",
    "            2. implement an epsilon-greedy policy with random exploration probability \n",
    "               passed in through the parameter `epsilon`.\n",
    "        \n",
    "        \"\"\"\n",
    "        x, y = state\n",
    "        q_s = self.q[y, x]\n",
    "        a = 0 # set a to the correct return value\n",
    "        return a\n",
    "        \n",
    "    \n",
    "    def render_pi(self, env, ax=None):\n",
    "        if ax is None:\n",
    "            fig, ax = env._create_fig()\n",
    "            \n",
    "        env.render_grid(ax)\n",
    "        for y in range(env.rows):\n",
    "            for x in range(env.cols):\n",
    "                env.render_action(ax, action=np.argmax(self.q[y, x]), agent=(x, y), length=0.3, hw=0.1, hl=0.15, ox=0, oy=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Q-Learning Update\n",
    "\n",
    "Implement the Q-Learning update $Q(s_t, a_t) \\leftarrow r + \\gamma \\, \\text{argmax}_j \\, Q(s_{t+1}, a_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning\n",
    "env = ExampleGrid()\n",
    "Q = np.zeros((env.rows, env.cols, len(env.action_space)))\n",
    "agent = QAgent(q=Q)\n",
    "epsilon = 1.0\n",
    "gamma = 0.95\n",
    "\n",
    "returns = []\n",
    "for i in range(200):\n",
    "    \n",
    "    # do training rollout with epsilon-greedy policy\n",
    "    states, actions, rewards = agent.rollout(env, epsilon)\n",
    "    G = np.sum(rewards * gamma**np.arange(len(rewards)))\n",
    "    returns.append(G)\n",
    "    \n",
    "    # update Q-Table\n",
    "    for t in range(len(states)-1):\n",
    "        x, y = states[t]\n",
    "        a = actions[t]\n",
    "        x_next, y_next = states[t+1]\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: implement the Q-Learning update.\n",
    "        \n",
    "        \"\"\"\n",
    "        Q[y, x, a] = Q[y, x, a] # an illustrative no-op that needs adapting.\n",
    "    \n",
    "    # decay epsilon\n",
    "    epsilon = max(0.05, 0.9*epsilon)\n",
    "\n",
    "# store returns for comparison\n",
    "returns_q = returns\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 24))\n",
    "ax[0].plot(returns_q)\n",
    "ax[0].set_title('Q-Learning training return of each rollout')\n",
    "ax[0].set_ylabel('Return')\n",
    "ax[0].set_xlabel('Rollout')\n",
    "agent.render_pi(env, ax[1])\n",
    "ax[1].set_title('Q-Learning learned policy after training.')\n",
    "env.render_rollout(states, ax=ax[2])\n",
    "ax[2].set_title('Q-Learning example rollout after training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation\n",
    "\n",
    "Evaluate your algorithm, training 5 times and plot mean, min and max return after each rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Does the task get harder to learn if the agent starts in a different random position?\n",
    "\n",
    "Adapt the environmnet such that the the agent starts at a different (random) position in each rollout and explore the training progress of one RL algorithm. You may need to adapt the number of rollouts, the learning rate, or the exploration rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleGrid(GridWorld):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        TODO: modify the assignment to `agent_start` below such that the agent starts \n",
    "              in a different random position (y, x) in each iteration.\n",
    "        \"\"\"\n",
    "        super().__init__(rows=4, cols=5, agent_start=(0, 3))\n",
    "        self.update_terminal(4, 0) # make state (4, 0) a terminal state\n",
    "        self.update_reward(4, 0, 1) # make transitions into state (4, 0) yield a reward of 1\n",
    "        self.update_reward(1, 2, -1) # make transitions into state (1, 2) yield a reward of -1\n",
    "        self.agent_start = agent_start\n",
    "        \n",
    "env = ExampleGrid() # instantiate the example grid\n",
    "env.render() # display the environment state(green dot), terminal states (gray shaded cells), and rewards (numbers inside cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Evaluate one RL algorithm of you choice on the modified environment. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore different tasks by modifying the environment's reward structure\n",
    "\n",
    "Change the reward structure of the environment to represent a different custom task of your choice and explore the training progress of one RL algorithm, and interpret the learned policy. You may need to adapt the number of rollouts, the learning rate, or the exploration rate (epsilon).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleGrid(GridWorld):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(rows=4, cols=5, agent_start=(0, 3))\n",
    "        self.update_terminal(4, 0) # make state (4, 0) a terminal state\n",
    "        \"\"\"\n",
    "        TODO: change the reward structure to represent a different custom task.\n",
    "        \"\"\"\n",
    "        self.update_reward(4, 0, 1) # make transitions into state (4, 0) yield a reward of 1\n",
    "        self.update_reward(1, 2, -1) # make transitions into state (1, 2) yield a reward of -1\n",
    "        \n",
    "env = ExampleGrid() # instantiate the example grid\n",
    "env.render() # display the environment state(green dot), terminal states (gray shaded cells), and rewards (numbers inside cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Evaluate one RL algorithm of you choice on the modified environment. \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
